{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Traffic Sign Detection - Project**\n",
    "\n",
    "- **Author**: Uyen Nguyen\n",
    "- **Date**: 2023/09/29\n",
    "- **Course**: AI Vietnam - Course 2023\n",
    "- **Module**: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **I. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Traffic Sign Dectection*** is a problem applied algorithms related to the field of Object Detection to detect traffic signs on road. Normally, a Traffic Sgin Detection program includes two parts, which includes locating the signs and recognizing the traffic sign. So, a high-accuracy program needs to well-build the two parts mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this porject, we will build a Traffic Sign Detection using Support Vector Machine (SVM). Defining the inpput and output of the program as follows:\n",
    "- **Input**: A picture of a traffic sign\n",
    "- **Output**: The location (location) and class of the sign in the picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **II. Program Installation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will start to build the program to detect different types of traffic signs. For the sake of the easeness of forming and logical thinking of building this program, this section will be split into two sections, corresponding to two main modules. These include \"*Building a classification model using SVM*\" and \"*Building an Object Detect model using sliding window technique*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Traffic Sign Classification Model Using SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **a. Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage import feature\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **b. Data Loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin to read and store picture files and associated labels into 2 different lists, corresponding to X and y in our traffic signs classification problem. Inside the data folder, we have two other folders:\n",
    "- **images**: Folder with pictures\n",
    "- **annotations**: Folder with .xml file, which is the label file, contains the information about coordinate and class of objects in the coressponding pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will declare the (relative) path to the two folders as well as two empty lists to store the pictures and labels as we're going through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the path to the two images and annotations folder\n",
    "annotations_dir = \"../data/traffic_sign_detection/annotations\"\n",
    "img_dir         = \"../data/traffic_sign_detection/images\"\n",
    "\n",
    "# Create two emmpty lists to store images and labels \n",
    "label_lst = []\n",
    "img_lst  = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code chunk contains of a few different steps to expand the `img_lst` and `label_lst` above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. First, we will browse through each .xml file in **annotations** folder. To browser every file name in a folder, we will use `os.listdir()` function. To create a complete path to the .xml file, we use `os.path.join(path1, path2)` to connect folder annotations and file name together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Examining a sample .xml file in the annotations folder, we can see the following information:\n",
    "\n",
    "    ```\n",
    "    <annotation>\n",
    "        <folder>images</folder>\n",
    "        <filename>road0.png</filename>\n",
    "        <size>\n",
    "            <width>267</width>\n",
    "            <height>400</height>\n",
    "            <depth>3</depth>\n",
    "        </size>\n",
    "        <segmented>0</segmented>\n",
    "        <object>\n",
    "            <name>trafficlight</name>\n",
    "            <pose>Unspecified</pose>\n",
    "            <truncated>0</truncated>\n",
    "            <occluded>0</occluded>\n",
    "            <difficult>0</difficult>\n",
    "            <bndbox>\n",
    "                <xmin>98</xmin>\n",
    "                <ymin>62</ymin>\n",
    "                <xmax>208</xmax>\n",
    "                <ymax>232</ymax>\n",
    "            </bndbox>\n",
    "        </object>\n",
    "    </annotation>\n",
    "    ```\n",
    "    Inside a .xml file, we will get multiple information about the picture, and the most important information that we want to focus on this project is coordination information and class name of the object (in this case is the traffic sign). So that, we will care about <**name**> and <**bndbox**> entities. Inside a <*object*> entity, <*name*> corresponds to its class and <*bnd*> gives information about its location (coordinate) in the picture. To read the content of a .xml file in Python, we can use this xml module as demonstrated in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The xml module also allow us to interact with different entities in a .xml file. After getting the root, we can search for/extract information of child entities of the root. For example, we can get the information of the <*folder*> entity to read the name of the image file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Similarly, we can get information about <*name*> of <*object*>. Because a single picture can contain different objects, we will use a loop to browse each object if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finally, we will get information about the coordinates of the <*bndbox*> and extract the object picture to store in the `img_lst`. We also save the `classname` in the `label_lst`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xml_file in os.listdir(annotations_dir):\n",
    "    # Step 1: Connect folder annotations and file name together\n",
    "    xml_filepath = os.path.join(annotations_dir, xml_file)\n",
    "\n",
    "    # Step 2: Implement the xml module to read the content of the .xml file\n",
    "    tree = ET.parse(xml_filepath)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Step 3: Use xml module to get the image file name and create a path to the corresponding image file\n",
    "    folder = root.find(\"folder\").text\n",
    "    img_filename = root.find(\"filename\").text\n",
    "    img_filepath = os.path.join(img_dir, img_filename)\n",
    "    img = cv2.imread(img_filepath)\n",
    "\n",
    "    # Step 4: Get information about the name of the object in the picture\n",
    "    #         Because we only care about traffic signs, we will pass the class \"trafficlight\" if encountered in the dataset\n",
    "    for obj in root.findall(\"object\"):\n",
    "        classname = obj.find(\"name\").text\n",
    "        if classname == \"trafficlight\":\n",
    "            continue\n",
    "        \n",
    "        # Step 5: Get information about the coordinate of the boundary box\n",
    "        xmin = int(obj.find(\"bndbox/xmin\").text)\n",
    "        ymin = int(obj.find(\"bndbox/ymin\").text)\n",
    "        xmax = int(obj.find(\"bndbox/xmax\").text)\n",
    "        ymax = int(obj.find(\"bndbox/ymax\").text)\n",
    "\n",
    "        # Extract the object inside the picture\n",
    "        object_img = img[ymin:ymax, xmin:xmax]\n",
    "\n",
    "        # Append the object extraction to the img_list\n",
    "        img_lst.append(object_img)\n",
    "\n",
    "        # Save the corresponding classname\n",
    "        label_lst.append(classname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the information about the `xmin`, `ymin`, `xmax`, `ymax`, we can easily extract the object out of the original picture by using slicing technique. Finally, we store cut object picture in `img_lst` and class name in `label_lst` as in the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sanity test, we will print out the number of objects and names of the classes inside the dataset used to train traffic sign detection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects: 1074\n",
      "Class names ['crosswalk', 'speedlimit', 'stop']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of objects:\", len(img_lst))\n",
    "print(\"Class names\", list(set(label_lst)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that out of all pictures in the given dataset, there are 1074 objects detected. The objects are of three different classes: `crosswalk`, `speedlimit`, and `stop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **c. Image Preprocessing Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the SVM model achieve better accuracy, we will procede to build a preprocessing function to preprocess the input picture and create a better form of representation for the images. Specifically, we would use **HOG (Histogram of Oriented Gradients)** feature in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create the HOG feature, we will use `feature.hog()` function inside `skimage` library. The preprocessing function can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    if len(img.shape) > 2:\n",
    "        img = cv2.cvtColor(\n",
    "            img,\n",
    "            cv2.COLOR_BGR2GRAY\n",
    "        )\n",
    "    img = img.astype(np.float32)\n",
    "\n",
    "    resized_img = resize(\n",
    "        img,\n",
    "        output_shape  = (32, 32),\n",
    "        anti_aliasing = True\n",
    "    )\n",
    "\n",
    "    hog_feature = feature.hog(\n",
    "        resized_img,\n",
    "        orientations = 9,\n",
    "        pixels_per_cell = (8, 8),\n",
    "        cells_per_block = (2, 2),\n",
    "        transform_sqrt = True,\n",
    "        block_norm = \"L2\",\n",
    "        feature_vector = True\n",
    "    )\n",
    "\n",
    "    return hog_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside HOG, we will also converse the picture into grayscale and change the size to (32, 32) before calculating HOG. Because objects have different sizes so uniforming the size is necessary for the HOG feature vector of all pictures are of the same shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **d. Preprocessing Inputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the `preprocess_img()`, we will now implement the function to preprocess all of the input pictures as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features_lst = []\n",
    "\n",
    "for img in img_lst:\n",
    "    hog_feature = preprocess_img(img)\n",
    "    img_features_lst.append(hog_feature)\n",
    "\n",
    "img_features = np.array(img_features_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sanity check, we will check the shape of the first picture in the list before and after preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first image before preprocessing: (42, 41, 3)\n",
      "Shape of the first image after preprocessing: (324,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the first image before preprocessing:\", img_lst[0].shape)\n",
    "print(\"Shape of the first image after preprocessing:\", img_features[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that before preprocess, the picture is a 42 x 41 (pixels) image with colors. However, after preprocess, the image has transformed into a single feature vector with 324 elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **e. Encode Labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, the labels are of type string (\"stop\", \"crosswalk\", \"speedlimit\"). We need to change the labels into numeric types for the purpose of model training. Here, we use `LabelEncoder()` to transform the class name into the corresponding 0, 1, 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(label_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **f. Dataset Spliting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the list of HOG feature input vectors (X) and corresponding label (y), we now procede to divide the dataset into 2 different sets, namely `train` for training data and `val` for validation data, with the ratio of 7:3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "test_size = 0.3\n",
    "is_shuffle = True\n",
    "\n",
    "X_train, X_val, y_train, y_val, = train_test_split(\n",
    "    img_features, encoded_labels,\n",
    "    test_size = test_size,\n",
    "    random_state = random_state,\n",
    "    shuffle = is_shuffle\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **g. Data Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ease of calculation of the training model, we normalize the data of the HOG feature vectors using `StandardScaler()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **h. Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of the necessary steps to prepare the datasets, we now move to train the SVM model using the `train` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=0.5, probability=True, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=0.5, probability=True, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=0.5, probability=True, random_state=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a Support Vector Machine (SVM) Classifier\n",
    "clf = SVC(\n",
    "    kernel = \"rbf\",\n",
    "    random_state = random_state,\n",
    "    probability = True,\n",
    "    C = 0.5\n",
    ")\n",
    "\n",
    "# Train the SVM Classifier\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **i. Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now evaluate the accuracy of the model on `val` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results on val set\n",
      "Accuracy: 0.978328173374613\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_val)\n",
    "score = accuracy_score(y_pred, y_val)\n",
    "\n",
    "print(\"Evaluation results on val set\")\n",
    "print(\"Accuracy:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
