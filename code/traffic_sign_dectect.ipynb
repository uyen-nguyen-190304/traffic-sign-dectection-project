{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Traffic Sign Detection - Project**\n",
    "\n",
    "- **Author**: Uyen Nguyen\n",
    "- **Date**: 2023/09/29\n",
    "- **Course**: AI Vietnam - Course 2023\n",
    "- **Module**: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **I. Introduction**\n",
    "\n",
    "***Traffic Sign Dectection*** is a problem applied algorithms related to the field of Object Detection to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage import feature\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the path to the two images and annotations folder\n",
    "annotations_dir = \"../data/traffic_sign_detection/annotations\"\n",
    "img_dir         = \"../data/traffic_sign_detection/images\"\n",
    "\n",
    "# Create two emmpty lists to store images and labels \n",
    "label_lst = []\n",
    "img_lst  = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will browse through each .xml file in **annotations** folder. To browser every file name in a folder, we will use `os.listdir()` function. To create a complete path to the .xml file, we use `os.path.join(path1, path2)` to connect folder annotations and file name together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xml_file in os.listdir(annotations_dir):\n",
    "    xml_filepath = os.path.join(annotations_dir, xml_file)\n",
    "\n",
    "    tree = ET.parse(xml_filepath)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    folder = root.find(\"folder\").text\n",
    "    img_filename = root.find(\"filename\").text\n",
    "    img_filepath = os.path.join(img_dir, img_filename)\n",
    "\n",
    "    # Load the image using OpenCV\n",
    "    img = cv2.imread(img_filepath)\n",
    "\n",
    "    for obj in root.findall(\"object\"):\n",
    "        classname = obj.find(\"name\").text\n",
    "        if classname == \"trafficlight\":\n",
    "            continue\n",
    "\n",
    "        xmin = int(obj.find(\"bndbox/xmin\").text)\n",
    "        ymin = int(obj.find(\"bndbox/ymin\").text)\n",
    "        xmax = int(obj.find(\"bndbox/xmax\").text)\n",
    "        ymax = int(obj.find(\"bndbox/ymax\").text)\n",
    "\n",
    "        object_img = img[ymin:ymax, xmin:xmax]\n",
    "        img_lst.append(object_img)\n",
    "        label_lst.append(classname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining a sample .xml file in the annotations folder, we can see the following information:\n",
    "\n",
    "```\n",
    "<annotation>\n",
    "    <folder>images</folder>\n",
    "    <filename>road0.png</filename>\n",
    "    <size>\n",
    "        <width>267</width>\n",
    "        <height>400</height>\n",
    "        <depth>3</depth>\n",
    "    </size>\n",
    "    <segmented>0</segmented>\n",
    "    <object>\n",
    "        <name>trafficlight</name>\n",
    "        <pose>Unspecified</pose>\n",
    "        <truncated>0</truncated>\n",
    "        <occluded>0</occluded>\n",
    "        <difficult>0</difficult>\n",
    "        <bndbox>\n",
    "            <xmin>98</xmin>\n",
    "            <ymin>62</ymin>\n",
    "            <xmax>208</xmax>\n",
    "            <ymax>232</ymax>\n",
    "        </bndbox>\n",
    "    </object>\n",
    "</annotation>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside a .xml file, we will get multiple information about the picture, and the most important information that we want to focus on this project is coordination information and class name of the object (in this case is the traffic sign). So that, we will care about <**name**> and <**bndbox**> entities. Inside a <*object*> entity, <*name*> corresponds to its class and <*bnd*> gives information about its location (coordinate) in the picture. To read the content of a .xml file in Python, we can use this xml module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of objects: 1074\n",
      "Class names ['crosswalk', 'stop', 'speedlimit']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of objects:\", len(img_lst))\n",
    "print(\"Class names\", list(set(label_lst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    if len(img.shape) > 2:\n",
    "        img = cv2.cvtColor(\n",
    "            img,\n",
    "            cv2.COLOR_BGR2GRAY\n",
    "        )\n",
    "    img = img.astype(np.float32)\n",
    "\n",
    "    resized_img = resize(\n",
    "        img,\n",
    "        output_shape  = (32, 32),\n",
    "        anti_aliasing = True\n",
    "    )\n",
    "\n",
    "    hog_feature = feature.hog(\n",
    "        resized_img,\n",
    "        orientations = 9,\n",
    "        pixels_per_cell = (8, 8),\n",
    "        cells_per_block = (2, 2),\n",
    "        transform_sqrt = True,\n",
    "        block_norm = \"L2\",\n",
    "        feature_vector = True\n",
    "    )\n",
    "\n",
    "    return hog_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features_lst = []\n",
    "\n",
    "for img in img_lst:\n",
    "    hog_feature = preprocess_img(img)\n",
    "    img_features_lst.append(hog_feature)\n",
    "\n",
    "img_features = np.array(img_features_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first image before preprocessing: (42, 41, 3)\n",
      "Shape of the first image after preprocessing: (324,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the first image before preprocessing:\", img_lst[0].shape)\n",
    "print(\"Shape of the first image after preprocessing:\", img_features[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
